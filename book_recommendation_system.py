# -*- coding: utf-8 -*-
"""Book Recommendation System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RKynMsKFeyvU3fJiwkvtcluyyh0rgHIq

#Import Libraries
"""

import pandas as pd
import gdown
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import numpy as np
from tensorflow import keras
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split

"""import library library yang diperlukan nantinya untuk pembuatan model

#Exploratory Data Analysis

##Load Dataset
"""

# URL Google Drive
users_url = 'https://drive.google.com/uc?id=1Ppo-eHXxPf_SsmIar7lcxVFB7dT3Y3dC'
ratings_url = 'https://drive.google.com/uc?id=1y7uCmxZpnITE701PJDTfOtz6ZXoJEgKi'
books_url = 'https://drive.google.com/uc?id=1JC_aDsDWmFcDPLmEmCNgJ_tyYfg6xBA5'

# Mengunduh file
gdown.download(users_url, 'Users.csv', quiet=False)
gdown.download(ratings_url, 'Ratings.csv', quiet=False)
gdown.download(books_url, 'Books.csv', quiet=False)

# Membaca file CSV menggunakan Pandas
users_df = pd.read_csv('Users.csv')
ratings_df = pd.read_csv('Ratings.csv')
books_df = pd.read_csv('Books.csv')

# Menampilkan beberapa baris dari masing-masing DataFrame
print("Users DataFrame:")
print(users_df.head())

print("\nRatings DataFrame:")
print(ratings_df.head())

print("\nBooks DataFrame:")
print(books_df.head())

"""import dataset dan check apakah dataset berhasil di import menggunakan function head()

##Users Variabel
"""

users_df.info()

"""check informasi user dataset. terdapat 3 kolom(user-id, location, dan age) dan 278853 baris data"""

print('Banyak User :' , len(users_df['User-ID'].unique()))
print('Location users :', users_df['Location'].unique())
print('rentang umur users :', users_df['Age'].unique())

"""check data kolom dari user dataset"""

users_df['Age'].isnull().sum()

"""check missing values kolom age"""

users_df.dropna(subset=['Age'], inplace=True)

users_df['Age'].isnull().sum()

"""hapus missing values"""

# Visualize Age Distribution
plt.figure(figsize=(10, 6))
sns.histplot(users_df['Age'], bins=30, kde=True)
plt.title('Age Distribution of Users')
plt.xlabel('Age')
plt.ylabel('Number of Users')
plt.show()

"""menampilkan peta penyebaran umur dari users yang me-rating buku. user paling banyak berada di kisaran 20-50 tahun"""

# Visualize Top 10 Locations
top_locations = users_df['Location'].value_counts().head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x=top_locations.index, y=top_locations.values)
plt.title('Top 10 User Locations')
plt.xlabel('Location')
plt.ylabel('Number of Users')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""memperlihatkan top 10 daerah users. menunjukkan bahwa mayoritas user yang memberi rating berada di daerah eropa

##Ratings Variabel
"""

ratings_df.info()

"""check informasi rating dataset. terdapat 3 kolom(user-id, book-id, dan rating) dan 1149780 baris data"""

print('Jumalh Jenis Buku yang dirating :', len(ratings_df['ISBN'].unique()))

"""check jumlah unique value dari ISBN"""

# Visualize Rating Distribution
plt.figure(figsize=(8, 6))
sns.countplot(x='Book-Rating', data=ratings_df)
plt.title('Distribution of Book Ratings')
plt.xlabel('Rating')
plt.ylabel('Number of Ratings')
plt.show()

"""memerplihatkan penyebaran rating buku. dapat terlihat bahwa banyak buku yang memiliki rating 0. disusul dengan rating 8 dan 10"""

# Menghitung jumlah rating per pengguna
ratings_per_user = ratings_df.groupby('User-ID')['Book-Rating'].count()
# Menghitung frekuensi dari jumlah rating
ratings_count = ratings_per_user.value_counts().sort_index()

# Membuat grafik garis
plt.figure(figsize=(10, 6))
plt.plot(ratings_count.index, ratings_count.values, marker='|')
plt.title('Number of Ratings per User')
plt.xlabel('Number of Ratings')
plt.ylabel('Number of Users')
plt.xlim(0, 750)
plt.grid()
plt.show()

"""memvisualisasikan distribusi jumlah rating yang diberikan oleh pengguna dalam dataset. mayoritas user cuma menulis sekitar < 50 rating. kebanyakan user memberi rating hanya sekali saja."""

# Analyze the relationship between rating and ISBN
plt.figure(figsize=(10, 6))
rating_counts = ratings_df['ISBN'].value_counts().head(20)
sns.barplot(x=rating_counts.index, y=rating_counts.values)
plt.title('Top 20 Books with the Most Ratings')
plt.xlabel('ISBN')
plt.ylabel('Number of Ratings')
plt.xticks(rotation=90)
plt.show()

"""memperlihatkan relasi antar isbn dan ratings. dan memperlihatkan buku mana saja yang mendapatkan jumlah rating terbanyak.

##Books Variabel
"""

books_df.info()

"""check informasi books dataset. terdapat 11 kolom dan 2193 baris data"""

print('jumlah judul buku :', len(books_df['Book-Title'].unique()))
print('jumlah penerbit buku :', len(books_df['Publisher'].unique()))
print('jumlah pengarang buku :', len(books_df['Book-Author'].unique()))
print('Tahun terbit buku-buku :', books_df['Year-Of-Publication'].unique())

"""check data kolom books_df. jenis buku bervariasi dari banyak pengarang, banyak penerbit, dan banyak tahun terbit buku"""

# Convert 'Year-Of-Publication' to numeric, coercing errors to NaN
books_df['Year-Of-Publication'] = pd.to_numeric(books_df['Year-Of-Publication'], errors='coerce')

# Now, you can safely convert to int, with NaNs remaining as NaNs
books_df['Year-Of-Publication'] = books_df['Year-Of-Publication'].astype('Int64') # Use 'Int64' for nullable integer type

print(books_df.info())

"""mengubah data tahun penerbitan buku menjadi int untuk mempermudah analisis"""

# Visualize the distribution of book publication years
plt.figure(figsize=(12, 6))
sns.histplot(books_df['Year-Of-Publication'], bins=30, kde=True)
plt.title('Distribution of Book Publication Years')
plt.xlabel('Year of Publication')
plt.ylabel('Number of Books')
plt.show()

"""memperlihatkan penyebaran data tahun penerbitan buku. memperlihatkan bahwa dataset memiliki data banyak buku yang diterbitkan di sekitar tahun 2000"""

# Visualize top publishers
top_publishers = books_df['Publisher'].value_counts().head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x=top_publishers.index, y=top_publishers.values)
plt.title('Top 10 Publishers')
plt.xlabel('Publisher')
plt.ylabel('Number of Books')
plt.xticks(rotation=90)
plt.show()

"""memperlihatkan penyebaran data publisher dengan menampilkan top 10 publisher yang menerbitkan buku terbanyak. terlihat bahwa harlequin memiliki buku yang paling banyak diterbitkan"""

# Visualize top authors
top_authors = books_df['Book-Author'].value_counts().head(10)
plt.figure(figsize=(12, 6))
sns.barplot(x=top_authors.index, y=top_authors.values)
plt.title('Top 10 Authors')
plt.xlabel('Author')
plt.ylabel('Number of Books')
plt.xticks(rotation=90)
plt.show()

"""memvisualisasikan top 10 author dengan buku terbanyak. agatha christie merupakan author yang memiliki buku terbanyak berdasarkan dataset

#Data Preprocessing
"""

ratings_books_df = pd.merge(ratings_df, books_df, on='ISBN', how='inner')

# Merge the result with users dataframe
merged_df = pd.merge(ratings_books_df, users_df, on='User-ID', how='inner')

merged_df.head()

"""menggabungkan dataset menjadi 1"""

merged_df.info()

"""pengecekan info dataset setelah penggabungan"""

merged_df.shape

"""#Data Preparation

##Handling Missing Values
"""

merged_df.isnull().sum()

# Drop missing values in columns other than 'Age'
merged_df = merged_df.dropna(subset=['ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L', 'Location', 'User-ID', 'Book-Rating'])
merged_df.isnull().sum()

"""##Handling Duplicate Data"""

merged_df.duplicated().sum()

# Drop duplicates (if any)
merged_df.drop_duplicates(inplace=True)

# Verify that duplicates have been removed
merged_df.duplicated().sum()

merged_df.shape

"""##Handling Outlier"""

merged_df.info()

# Check and visualize outliers using boxplots for 'Book-Rating', 'Year-Of-Publication', and 'Age'
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.boxplot(y=merged_df['Book-Rating'])
plt.title('Book-Rating Outliers')

plt.subplot(1, 3, 2)
sns.boxplot(y=merged_df['Year-Of-Publication'])
plt.title('Year-Of-Publication Outliers')

plt.subplot(1, 3, 3)
sns.boxplot(y=merged_df['Age'])
plt.title('Age Outliers')

plt.tight_layout()
plt.show()

# Calculate IQR for 'Age'
Q1_age = merged_df['Age'].quantile(0.25)
Q3_age = merged_df['Age'].quantile(0.75)
IQR_age = Q3_age - Q1_age
lower_bound_age = Q1_age - 1.5 * IQR_age
upper_bound_age = Q3_age + 1.5 * IQR_age

# Filter out outliers for 'Age'
merged_df = merged_df[~((merged_df['Age'] < lower_bound_age) | (merged_df['Age'] > upper_bound_age))]

# Calculate IQR for 'Year-Of-Publication'
Q1_year = merged_df['Year-Of-Publication'].quantile(0.25)
Q3_year = merged_df['Year-Of-Publication'].quantile(0.75)
IQR_year = Q3_year - Q1_year
lower_bound_year = Q1_year - 1.5 * IQR_year
upper_bound_year = Q3_year + 1.5 * IQR_year

# Filter out outliers for 'Year-Of-Publication'
merged_df = merged_df[~((merged_df['Year-Of-Publication'] < lower_bound_year) | (merged_df['Year-Of-Publication'] > upper_bound_year))]

# After outlier handling, you might want to recheck the distributions
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.boxplot(y=merged_df['Year-Of-Publication'])
plt.title('Year-Of-Publication Outliers (After Handling)')

plt.subplot(1, 2, 2)
sns.boxplot(y=merged_df['Age'])
plt.title('Age Outliers (After Handling)')

plt.tight_layout()
plt.show()

merged_df.shape

"""#Model Development dengan Content Based Filtering"""

# Group data by 'Book-Title' and aggregate relevant columns
grouped_df = merged_df.groupby('Book-Title').agg({
    'Book-Author': 'first',  # Keep the first author
    'Year-Of-Publication': 'first',
    'Publisher': 'first',
    'Book-Rating': 'mean',  # Calculate average rating
    'User-ID': 'count'  # Count number of ratings
}).reset_index()

# Filter out books with less than a certain number of ratings (adjust threshold as needed)
min_ratings_threshold = 5
filtered_df = grouped_df[grouped_df['User-ID'] >= min_ratings_threshold]

# Sort by average rating (optional)
filtered_df = filtered_df.sort_values(by='Book-Rating', ascending=False)

# Reset index after filtering and sorting
filtered_df.reset_index(drop=True, inplace=True)

print(filtered_df.head())
filtered_df.shape

filtered_df['Book-Rating'].nunique()

# Create a TF-IDF vectorizer to convert text data into numerical vectors
tfidf = TfidfVectorizer(stop_words='english')

# Combine relevant text columns into a single column for TF-IDF vectorization
filtered_df['combined_features'] = filtered_df['Book-Author'] + ' ' + filtered_df['Publisher']

# Fit and transform the combined features to create TF-IDF vectors
tfidf_matrix = tfidf.fit_transform(filtered_df['combined_features'])

# Calculate the cosine similarity between books based on their TF-IDF vectors
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Function to get book recommendations based on book title
def get_recommendations(title, cosine_sim=cosine_sim):
    # Get the index of the book that matches the title
    idx = filtered_df[filtered_df['Book-Title'] == title].index[0]

    # Get the pairwise similarity scores of all books with that book
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the books based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get the scores of the 10 most similar books
    sim_scores = sim_scores[1:11]

    # Get the book indices
    book_indices = [i[0] for i in sim_scores]

    # Return the top 10 most similar books
    return filtered_df['Book-Title'].iloc[book_indices]

get_recommendations('Harry Potter and the Prisoner of Azkaban (Book 3)')

"""#Model Development dengan Collaborative Filtering"""

# 1.  User and Item Filtering:
#     a.  Filter out users with very few ratings:
user_rating_counts = merged_df['User-ID'].value_counts()
active_users = user_rating_counts[user_rating_counts >= 5].index  # Example threshold
merged_df = merged_df[merged_df['User-ID'].isin(active_users)]

#     b.  Filter out books with very few ratings:
book_rating_counts = merged_df['ISBN'].value_counts()
popular_books = book_rating_counts[book_rating_counts >= 5].index  # Example threshold
merged_df = merged_df[merged_df['ISBN'].isin(popular_books)]

# 2. Data Type Optimization:
#     Convert relevant columns to categorical types if appropriate.
#     This can significantly reduce memory usage.
for col in ['User-ID', 'ISBN']:
  merged_df[col] = merged_df[col].astype('category')

# 3.  Rating Distribution Analysis (optional but recommended):
#     a.  Explore if there are significant rating biases (e.g., many ratings at a specific value)
#     b.  Consider normalization or transformation of ratings if necessary (e.g., standardization).
merged_df['Book-Rating'] = (merged_df['Book-Rating'] - merged_df['Book-Rating'].min()) / (merged_df['Book-Rating'].max() - merged_df['Book-Rating'].min())

# 4.  Create User-Item Interaction Matrix (sparse matrix)
user_ids = merged_df['User-ID'].cat.codes.unique()
book_ids = merged_df['ISBN'].cat.codes.unique()

# Create a sparse matrix
import scipy.sparse as sparse

rows = merged_df['User-ID'].cat.codes
cols = merged_df['ISBN'].cat.codes
ratings = merged_df['Book-Rating']

user_item_matrix = sparse.csr_matrix((ratings, (rows, cols)), shape=(len(user_ids), len(book_ids)))

# Define the RecommenderNet model
def RecommenderNet(num_users, num_books, embedding_size, **kwargs):
    user_input = keras.layers.Input(shape=(1,), name='user_input')
    book_input = keras.layers.Input(shape=(1,), name='book_input')

    user_embedding = keras.layers.Embedding(
        input_dim=num_users,
        output_dim=embedding_size,
        name='user_embedding'
    )(user_input)
    book_embedding = keras.layers.Embedding(
        input_dim=num_books,
        output_dim=embedding_size,
        name='book_embedding'
    )(book_input)

    user_vec = keras.layers.Flatten(name='user_flatten')(user_embedding)
    book_vec = keras.layers.Flatten(name='book_flatten')(book_embedding)

    merged = keras.layers.concatenate([user_vec, book_vec], name='concatenate')

    hidden1 = keras.layers.Dense(128, activation='relu', name='hidden1')(merged)
    dropout1 = keras.layers.Dropout(0.2, name='dropout1')(hidden1)
    hidden2 = keras.layers.Dense(64, activation='relu', name='hidden2')(dropout1)
    dropout2 = keras.layers.Dropout(0.2, name='dropout2')(hidden2)
    output = keras.layers.Dense(1, activation='linear', name='output')(dropout2)  # Linear activation for regression

    model = keras.Model(inputs=[user_input, book_input], outputs=output)
    return model

# Prepare training data
X = []
y = []
for i in range(user_item_matrix.shape[0]):
  for j in range(user_item_matrix.shape[1]):
    if user_item_matrix[i, j] != 0:
      X.append([i, j])
      y.append(user_item_matrix[i, j])
X = np.array(X)
y = np.array(y)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Instantiate the RecommenderNet model
model = RecommenderNet(num_users=len(user_ids), num_books=len(book_ids), embedding_size=50)

# Compile the model
model.compile(loss='mse', optimizer='adam')

# Train the model
history = model.fit(x={'user_input': X_train[:, 0], 'book_input': X_train[:, 1]},
                    y=y_train,
                    epochs=5,  # Adjust epochs as needed
                    batch_size=64,  # Adjust batch size as needed
                    validation_data=({'user_input': X_test[:, 0], 'book_input': X_test[:, 1]}, y_test))

# Evaluate the model
loss = model.evaluate(x={'user_input': X_test[:, 0], 'book_input': X_test[:, 1]}, y=y_test)
print("Test Loss:", loss)

# Plot training & validation loss values
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Make predictions (example)
user_id_to_predict = 0

# Ensure user_input is a batch (even if of size 1)
user_input = np.array([[user_id_to_predict]])  # Wrap in an extra dimension to create a batch of size 1
book_input = np.arange(len(book_ids))

# Since we want to test each book for the user we need to do this in batches as well.
predicted_ratings = model.predict({'user_input': np.repeat(user_input, len(book_ids), axis=0), 'book_input': book_input})
predicted_ratings

users_df['User-ID'].head()

import numpy as np
import pandas as pd
from tensorflow import keras

# Get the user and book IDs for User-ID 44
user_id = 99
user_index = merged_df[merged_df['User-ID'] == user_id]['User-ID'].cat.codes.values[0]

# Get all book indices
all_book_indices = np.arange(len(book_ids))

# Find books the user has already rated
rated_books = merged_df[merged_df['User-ID'] == user_id]['ISBN'].cat.codes.values

# Find books the user hasn't rated yet
unrated_books = np.setdiff1d(all_book_indices, rated_books)

# Prepare input data for prediction
user_input = np.full(len(unrated_books), user_index)
book_input = unrated_books

# Predict ratings for unrated books
predicted_ratings = model.predict({'user_input': user_input, 'book_input': book_input}, verbose=0)

# Create a DataFrame of book indices and their predicted ratings
recommendations = pd.DataFrame({'book_index': book_input, 'predicted_rating': predicted_ratings.flatten()})

# Sort recommendations by predicted rating (descending)
recommendations = recommendations.sort_values(by='predicted_rating', ascending=False)

# Get the top 10 book recommendations
top_10_books = recommendations.head(10)

# Map book indices back to ISBNs
top_10_books['ISBN'] = top_10_books['book_index'].map(lambda x: merged_df['ISBN'].cat.categories[x])

# Join with the books data to get book titles
top_10_books = top_10_books.merge(books_df, on='ISBN', how='inner')

# Show top 10 recommended books
top_10_books[['ISBN', 'Book-Title', 'predicted_rating']]